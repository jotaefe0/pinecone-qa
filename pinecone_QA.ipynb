{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4614f6f-3b7b-49ef-83e9-9ab1992f82eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load libs and define some helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56d32689-d2c5-4e9a-a4b1-543697201b33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def show(img):\n",
    "    image = Image.open(img)\n",
    "    new_size = (600, 400)\n",
    "    resized_image = image.resize(new_size)\n",
    "    display(resized_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7246a24a-20c0-47e1-97ab-817f8b8f6d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b18c5b-d394-4869-a645-308181eab33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbb817-c1a8-4206-90d4-0d15f881ddf5",
   "metadata": {},
   "source": [
    "Let's load a Andrej K blog post about training NN (https://karpathy.github.io/2019/04/25/recipe/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a8d6186-4cc5-4454-939e-e951c9a64603",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('karpathy-recipe-nn.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c2f8030-5909-46fa-bb98-cd2fa65f6728",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a781b765-d5b1-4c62-98e3-8b4c2a6ffbbe",
   "metadata": {},
   "source": [
    "# QA for big documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd88a9-063e-486e-903e-bed2e51b5428",
   "metadata": {},
   "source": [
    "If we want to provide context to a prompt where the information is in a large corpus of text, we are limited by:\n",
    "\n",
    "* Limitations on the maximum number of tokens\n",
    "* High processing costs\n",
    "* Unnecessary processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e3f212-2f8f-46e3-b0d4-66a40852ae18",
   "metadata": {},
   "source": [
    "There is a strategy where we divide the text into smaller fragments or `chunks` and convert them into embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f717b860-b03d-40dd-9245-8e3c34bfdbe9",
   "metadata": {},
   "source": [
    "_An embedding is a numerical representation of an object or entity in a vector space. In the context of natural language processing, an embedding is used to represent words or phrases as vectors of real numbers. These vectors are designed in such a way that similar words or phrases in terms of meaning are close together in the vector space. Embeddings are used in tasks such as text classification, machine translation, text generation, and semantic search._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a17582-fa1e-4fbe-837c-90bc306fd67a",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"text-align:center\">\n",
    "<img src=\"./embedding.png\" alt=\"embedding\" width=\"600\" height=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03356e1a-c3de-47bf-b1c3-389a53d63f91",
   "metadata": {},
   "source": [
    "In our case, we will use the embeddings from OPENAI, specifically the `text-embedding-ada-002` model, which is very efficient for this purpose and has low costs.\n",
    "To avoid reprocessing the text every time we need its embeddings, we can store them in VDB (Vector Data Bases).\n",
    "In this example, we will use `Pinecone`, which is a Cloud service.\n",
    "The pipeline would look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a3c9c-1d34-4902-8a06-3fe6d32a36ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div style=\"text-align:center\">\n",
    "<img src=\"./docQA.png\" alt=\"embedding\" width=\"600\" height=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d875a71-eb31-4ceb-9264-c0134acd6c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pinecone\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aca58a-7b64-49bb-8fd9-13a1cb87fe3b",
   "metadata": {},
   "source": [
    "We create the index in Pinecone.\n",
    "\n",
    "In dimensions, we need to provide the length of our vector, for the case of `text-embedding-ada-002`, it returns a vector of dimension 1536."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "471f480a-8a4e-4672-aa4e-4d6e5122dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ae3dcaf-bed7-42c2-9087-757bb2013941",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = embeddings.embed_query(\"Hello world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531f6df7-0c39-4e7e-80a1-1e1fd8686282",
   "metadata": {},
   "source": [
    "We can check that the embedding dim is 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a5e526a-86fb-49bf-be15-53e56f5c6d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c86dcb6e-ee70-4756-b5ac-405bf477e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key=os.environ.get('PINECONE_KEY'),\n",
    "    environment=\"northamerica-northeast1-gcp\"\n",
    ")\n",
    "\n",
    "\n",
    "index_name = \"blog-summary\"\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # if does not exist, create index\n",
    "    pinecone.create_index(\n",
    "        index_name,\n",
    "        dimension=len(query_result),#1536\n",
    "        metric='cosine',\n",
    "        metadata_config={'indexed': ['channel_id', 'published']}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a215cd2f-9206-4346-8c84-1170a08b0df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "date = datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aea8a8-fbe9-45b1-ad3a-0bf3d265f249",
   "metadata": {},
   "source": [
    "If we use Lanchain we provide the `Document` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b24a5b25-a4d8-4e71-9bfe-8f1e166c0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the document object mannualy in this case\n",
    "documents = [Document(page_content=text,\n",
    "         metadata={\n",
    "             'my_document_id' : 1,\n",
    "             'my_document_source' : \"Jetta Rewiew\",\n",
    "             'my_document_create_time' : int(date.timestamp())\n",
    "         })]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bacfa97-eaa8-4b95-b645-8e7859c71368",
   "metadata": {},
   "source": [
    "We proceed to perform a split. In this case, we divide the text into chunks of 1000 tokens with an overlap of 20. The chunk_overlap parameter is used to specify the number of tokens that overlap between consecutive chunks. This is useful when splitting a text to maintain continuity of context between the chunks. By including some overlapping tokens, we can ensure that a small portion of the context is shared between adjacent chunks, which can help preserve meaning and coherence when processing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ed49d61-568d-4526-83cb-8f90aae94ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "def split_docs(documents, chunk_size=1000, chunk_overlap=20):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs\n",
    "\n",
    "docs = split_docs(documents)\n",
    "print(len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1c1c8cc-7234-486a-968c-8f52e5dc0217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Recipe for Training Neural Networks Apr 25, 2019  Some few weeks ago I posted a tweet on “the most common neural net mistakes”, listing a few common gotchas related to training neural nets. The tweet got quite a bit more engagement than I anticipated (including a webinar :)). Clearly, a lot of people have personally encountered the large gap between “here is how a convolutional layer works” and “our convnet achieves state of the art results”.  So I thought it could be fun to brush off my dusty blog to expand my tweet to the long form that this topic deserves. However, instead of going into an enumeration of more common errors or fleshing them out, I wanted to dig a bit deeper and talk about how one can avoid making these errors altogether (or fix them very fast). The trick to doing so is to follow a certain process, which as far as I can tell is not very often documented. Let’s start with two important observations that motivate it.  1) Neural net training is a leaky abstraction It'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0fd2fe-f2a8-4497-b8ea-fec0b78cf03f",
   "metadata": {},
   "source": [
    "Now we can store the embeddings in our VDB, which will generally have the following structure:\n",
    "`(ID, [embedding], metadata)`\n",
    "    \n",
    "    Where the ID is unique, the embedding is a vector of dimension 1536 (in our case), and the metadata can be a JSON-like object accompanying the vector. We can include the represented text as well as other attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70f6159c-9916-4e5d-a8cf-103fafb9be6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 11}},\n",
       " 'total_vector_count': 11}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to index\n",
    "index = pinecone.Index(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a253140-3892-47c5-980b-add1dc6f0a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91cc5922-a67c-48ae-bbe2-8bc5c739a02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text-embedding-ada-002'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d074176-5292-449b-b65f-d5d94c5c9fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = Pinecone.from_documents(docs, embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74ede6c-2843-4499-8a1d-38cd24c93809",
   "metadata": {},
   "source": [
    "We can estimate costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64806ac7-8384-43a6-891d-26321aa4a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cost per 1000 tokens\n",
    "# https://openai.com/pricing\n",
    "gpt35turbo_cost = 0.002\n",
    "ada_embeddings_cost = 0.0004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cfa82370-2189-4d94-8502-295ee8524be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est. cost : 0.0019 $USD\n"
     ]
    }
   ],
   "source": [
    "print(f'Est. cost : {sum([num_tokens_from_string(d.page_content,\"cl100k_base\") for d in docs]) *  0.001 * ada_embeddings_cost:.4f} $USD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869d6b74-549e-48c7-88f3-ccac9ace153f",
   "metadata": {},
   "source": [
    "At this point, we already have our document stored in the database, and we are left with the final part.\n",
    "\n",
    "If we want to ask a question about that document, we can make a query where:\n",
    "\n",
    "* We need to process our question (Embedding).\n",
    "* We will search in Pinecone for those chunks that are semantically similar to our question (we can request the top k nearest results).\n",
    "* Once we have those results, we can create a prompt as we would normally do, but only on that specific fragment of text where the answer to our question should be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1bac0854-6233-4e89-8575-68cbe31c5289",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 3750\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "def retrieve(query):\n",
    "    while True:\n",
    "        try:\n",
    "            res = openai.Embedding.create(\n",
    "                input=[query],\n",
    "                engine='text-embedding-ada-002'\n",
    "            )\n",
    "            break\n",
    "        except openai.error.APIConnectionError as e:\n",
    "            print(f'{e} Retrying...')\n",
    "            continue\n",
    "\n",
    "    \n",
    "    # retrieve from Pinecone\n",
    "    xq = res['data'][0]['embedding']\n",
    "    \n",
    "    \n",
    "\n",
    "    # get relevant contexts\n",
    "    res = index.query(xq, top_k=3, include_metadata=True)\n",
    "    contexts = [\n",
    "        x['metadata']['text'] for x in res['matches']\n",
    "    ]\n",
    "    \n",
    "\n",
    "    # build our prompt with the retrieved contexts included\n",
    "    prompt_start = (\n",
    "        \"Answer the question based on the context below.\\n\\n\"+\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # append contexts until hitting limit\n",
    "    for i in range(1, len(contexts)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(contexts)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ae3de7-8bfd-455e-90a6-a3a97abda732",
   "metadata": {},
   "source": [
    "We choose the 3 clostest docs to answer the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fcd98896-cd5f-4e25-a312-62c155887702",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the question based on the context below.\\n\\nContext:\\nto a new problem, which I will try to describe. You will see that it takes the two principles above very seriously. In particular, it builds from simple to complex and at every step of the way we make concrete hypotheses about what will happen and then either validate them with an experiment or investigate until we find some issue. What we try to prevent very hard is the introduction of a lot of “unverified” complexity at once, which is bound to introduce bugs/misconfigurations that will take forever to find (if ever). If writing your neural net code was like training one, you’d want to use a very small learning rate and guess and then evaluate the full test set after every iteration.  1. Become one with the data The first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data. This step is critical. I like to spend copious amount of time (measured in units of hours) scanning through thousands of examples,\\n\\n---\\n\\nfrom. And if your network is giving you some prediction that doesn’t seem consistent with what you’ve seen in the data, something is off.  Once you get a qualitative sense it is also a good idea to write some simple code to search/filter/sort by whatever you can think of (e.g. type of label, size of annotations, number of annotations, etc.) and visualize their distributions and the outliers along any axis. The outliers especially almost always uncover some bugs in data quality or preprocessing.  2. Set up the end-to-end training/evaluation skeleton + get dumb baselines Now that we understand our data can we reach for our super fancy Multi-scale ASPP FPN ResNet and begin training awesome models? For sure no. That is the road to suffering. Our next step is to set up a full training + evaluation skeleton and gain trust in its correctness via a series of experiments. At this stage it is best to pick some simple model that you couldn’t possibly have screwed up somehow - e.g. a linear\\n\\n---\\n\\nyou can’t afford the computation at test time look into distilling your ensemble into a network using dark knowledge. leave it training. I’ve often seen people tempted to stop the model training when the validation loss seems to be leveling off. In my experience networks keep training for unintuitively long time. One time I accidentally left a model training during the winter break and when I got back in January it was SOTA (“state of the art”). Conclusion Once you make it here you’ll have all the ingredients for success: You have a deep understanding of the technology, the dataset and the problem, you’ve set up the entire training/evaluation infrastructure and achieved high confidence in its accuracy, and you’ve explored increasingly more complex models, gaining performance improvements in ways you’ve predicted each step of the way. You’re now ready to read a lot of papers, try a large number of experiments, and get your SOTA results. Good luck!\\n\\nQuestion: What is the first step in this recipe?\\nAnswer:'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the first step in this recipe?\"\n",
    "query_with_contexts = retrieve(query)\n",
    "query_with_contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da626c3-e9dc-469c-ac90-666e97f30938",
   "metadata": {},
   "source": [
    "<br>\n",
    "Finnaly we pass the prompt to `text-davinci-003` (We could use `gpt-3.5-turbo` also)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e632e58d-7482-4c29-9199-e8f7b72b9805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(prompt):\n",
    "    # query text-davinci-003\n",
    "    res = openai.Completion.create(\n",
    "        engine='text-davinci-003',\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=400,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    return res['choices'][0]['text'].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06f35d98-f145-4659-a0df-b8872a4de030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The first step is to become one with the data by thoroughly inspecting it.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete(query_with_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03435d-1f73-4462-8efc-364be6ea7e41",
   "metadata": {},
   "source": [
    "### More:\n",
    "\n",
    "No Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0661ccac-13b7-4689-a473-8ed0a70e2440",
   "metadata": {},
   "source": [
    "This is the equivalent to `Pinecone.from_documents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8117fcf9-e76e-4884-b1db-7d1857b830d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from time import sleep\n",
    "\n",
    "batch_size = 100  # how many embeddings we create and insert at once\n",
    "\n",
    "for i in tqdm(range(0, len(new_data), batch_size)):\n",
    "    # find end of batch\n",
    "    i_end = min(len(new_data), i+batch_size)\n",
    "    meta_batch = new_data[i:i_end]\n",
    "    # get ids\n",
    "    ids_batch = [x['id'] for x in meta_batch]\n",
    "    # get texts to encode\n",
    "    texts = [x['text'] for x in meta_batch]\n",
    "    # create embeddings (try-except added to avoid RateLimitError)\n",
    "    try:\n",
    "        res = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "    except:\n",
    "        done = False\n",
    "        while not done:\n",
    "            sleep(5)\n",
    "            try:\n",
    "                res = openai.Embedding.create(input=texts, engine=embed_model)\n",
    "                done = True\n",
    "            except:\n",
    "                pass\n",
    "    embeds = [record['embedding'] for record in res['data']]\n",
    "    # cleanup metadata\n",
    "    meta_batch = [{\n",
    "        'start': x['start'],\n",
    "        'end': x['end'],\n",
    "        'title': x['title'],\n",
    "        'text': x['text'],\n",
    "        'url': x['url'],\n",
    "        'published': x['published'],\n",
    "        'channel_id': x['channel_id']\n",
    "    } for x in meta_batch]\n",
    "    to_upsert = list(zip(ids_batch, embeds, meta_batch))\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a20cfec7-c963-4500-b3d1-27df35e162e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "stride = 100\n",
    "window = 100\n",
    "\n",
    "for i in range(0,len(text.split(' ')),stride):\n",
    "    chunks.append(' '.join(text.split(' ')[i:window+i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd2fdd7-59f0-4413-99a3-0d6749dd064c",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "https://docs.pinecone.io/docs/query-data\n",
    "\n",
    "https://platform.openai.com/docs/guides/embeddings/what-are-embeddings\n",
    "\n",
    "https://python.langchain.com/en/latest/modules/indexes/vectorstores/examples/pinecone.html\n",
    "\n",
    "https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d7f77-5d60-4a50-bac1-e184966e0724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
